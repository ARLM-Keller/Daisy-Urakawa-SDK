
= Data Model: food for thoughts =

== General API framework design concerns ==

Statically-typed languages vs dynamic ones and duck-typing. Interfaces vs Abstract Classes:

http://haacked.com/archive/2008/02/21/versioning-issues-with-abstract-base-classes-and-interfaces.aspx

Sealing / Finalizing classes:

http://codebetter.com/blogs/patricksmacchia/archive/2008/01/05/rambling-on-the-sealed-keyword.aspx

Interesting article about method scoping/visibility in C#, and how to implement a missing language feature (the "Internal *And* Protected Virtual"):

http://agsmith.wordpress.com/2007/12/15/internal-and-protected-virtual/

Coming from the Java world, here is a good summary of the meaning of the visibility / scope keywords:

http://msdn.microsoft.com/en-us/library/ms173121.aspx

http://java.sun.com/docs/books/tutorial/java/javaOO/accesscontrol.html

http://msdn.microsoft.com/en-us/library/ms228632.aspx

Because of these language differences, there are discrepancies between the Java and C# Urakawa SDK API, especially with regards to calling methods across namespaces/packages. However, this is very unlikely to practically affect an application programmer in the real world, so we can live with that.

== !CheckedExceptions ==

in C#, there are no Checked Exceptions.

In the Java code of the Urakawa SDK however, we use them a lot to define certain parts of the API contract (pre-conditions for method parameters). At design-stage, having the compiler complaining about missing throw/catch clauses helps remembering the layering of the API (hierarchy of method calls), and the corresponding cascading of potential errors. This mental visualization is obviously facilitated in Eclipse thanks to the code being compiled on the fly.

Now, in a production environment, a Java build of the Urakawa SDK should be generated with a minimum of !CheckedExceptions, due to the runtime overhead created otherwise for nearly every single method of the toolkit.

There is a compelling rationale against the use of Checked Exceptions, explained here:

http://www.artima.com/intv/handcuffs.html 

== DITA vs !DocBook ==

DITA is an OASIS standard implemented by all the major XML editors.

Like DocBook, it's based on the single-source ("master") publishing paradigm. However, DITA has provisions in its authoring process for specializing the XML grammar while ensuring interoperability with the toolkit (therefore minimizing the production costs).


"
DITA builds content reuse into the authoring process, defining an XML architecture for designing, writing, managing, and publishing many kinds of information in print and on the Web.
"

"
But, why DITA?

Well, assuming your content fits into the topic-based data model, DITA's increasing popularity means that more and more authoring and publishing tools will be developed to support that model. The DITA Open Toolkit allows you to generate many popular output formats (HTML, HTML Help, PDF, Java Help, etc.) from DITA-based content. If you develop your own data model, you'll have to pay to develop those transformations. DITA's modular architecture, supports efficient reuse of content at the word, phrase or topic level. DITA also has the concept of "specialization," which allows you to develop elements of your own that are based on core DITA elements. This helps you to customize DITA to support your particular types of content while continuing to take advantage of the base DITA tools and transformations.
"

DITA encourages extensions:

It's a topic of interest for the Urakawa-SDK peoples, as we have been discussing ways of exchanging XUK content between different applications while preserving data model customizations (i.e. preserve as much of the structure of an Obi document when imported into Tobi).

"Extension is encouraged, but new elements must be recognizable as specializations of existing elements. Through generalization, DITA provides for tool reuse and interoperability."

http://www.ibm.com/developerworks/xml/library/x-dita3/#N257 

The topic-based authoring principle is described here:

http://dita.xml.org/topic-based-authoring 

In the light of all the discussions at the moment regarding DTBook "one-size-fit-all" at the publisher level, here's a thorough analysis with DITA:

"
Specialization is the answer to those who say DITA's "one-size-fits-all" collection of DTDs and schemas will not fit your organization's requirements and business case.

...

And if you follow the specialization guidelines exactly, you know that your DITA content can be exported to systems that know nothing about your specialization and it will be processed without error.
"

http://www.ditausers.org/about_us/business_case/ 
http://dita.xml.org/wiki/is-standardization-important
http://dita.xml.org/wiki/making-the-business-case-for-dita 

DITA establishes a distinction between specialization in content, design and processing:

http://www.ditawiki.org/index.php/DITA_specialization 

The "mapping" with an object-oriented data model is explicit in the DITA design:

http://www-128.ibm.com/developerworks/xml/library/x-dita1/#h4

In the Urakawa-SDK 2.0, we are trying to implement XUK polymorphism correctly, using a recursive fallback mechanism along the chain of (single) inheritance for XukAble objects.

Illustration:

!MyCustomTodoProperty -> !TodoProperty -> Property

or:

!FrenchTextAudioTreeNode -> !TextAudioTreeNode -> !TreeNode

Here's how it works in DITA:

http://www.ibm.com/developerworks/xml/library/x-dita2/#m13

They use the "class" attribute to specify the inheritance chain for a specialized ("custom") element. This way, the element can be generalized by a generic processor, or semantically picked-up in its entirety by a specialized processor.

This generalization mechanism is pretty much what the Tobi XUK processor would do when reading an Obi document, or vice-versa (assuming both Obi and Tobi are built on top of the new SDK 2.0, of course).

Possible interest for the Pipeline folks: Eliot Kimber is a member of the OASIS-DITA group, and he seems to be working on a DITA to DTBook XSLT.

"Designing and developing XSLT scripts for eduational publisher to meet
the National Instructional Materials Accessibility Standard (NIMAS)."

http://www.reallysi.com/bioek.htm 

== Events ==

The Mozilla event mechanism is specified in the XPCOM API using IDL, and is based on the Event-Listener variant of the Publish/Subscribe design pattern (a listener interface is a set of semantically-related event callback methods, and listener registration applies to the whole set).

As a result, it looks very similar to the Urakawa Java-written architecture and can be easily "mentally-translated" to the C# native eventargs/evetnhandler/delegate mechanism.

The same could be said for DOM[3], although because of its generic nature, DOM exposes an Event-Bus design (single callback method with a base event type) rather than specific listener methods on a per-event-type basis. The filtering that is performed during event dispatch is based on a flag which is specified at callback-registration time.

(1) The "bus" design pattern requires a hierarchy of event types, and the event instances contain all the data related to the event. By contrast, the "listener" design pattern requires a well-defined set of callback methods (interface) to cater for each type of events.

The latter corresponds directly to the C# native event model, but the former is more suitable for generic events that bubble-up in a tree structure. Because we are now improving the event mechanism in the Urakawa SDK (to enable per-TreeNode listener registration), we need to cater for bubbling issues and event genericity.

Ole is currently writing a C#-specific implementation to see how the native event/delegate mechanism can be used. The language-agnostic architecture (expressed in Java) could be specified in different ways (bus, listener, or both), but we want to make sure it is easily translatable in vanilla C#.

(2) Naming conventions: the "OnXXXEvent()" naming scheme for callback methods is widely used. Should we use this in the Urakawa SDK ? (e.g. OnTreeNodeRemoved(...))

(3) Exceptions: there are inconsistencies in the Mozilla code base (e.g. un/registration exceptions are declared with iWebProgress[4], but not with iDownloadProgress[5]). On the other hand, DOM does not declare any exception in its IDL specification (for the particular un/registration scenario), but I am happy to see that DOM checks[6] for empty or null DOMString (this is what we thoroughly do in the Urakawa SDK).

I am in favor of declaring exceptions for redundant registration and failed unregistration, but I have no strong feelings (as long as it's consistent).

(4) Finally, a note to Ole: let's not forget that the cloning operation on a node does not copy the registered listeners (which I think is the most sensible thing to do, as per the DOM specification).

[3] http://www.w3.org/TR/DOM-Level-2-Events/events.html
[4] http://developer.mozilla.org/en/docs/nsIWebProgress#addProgressListener
[5] http://developer.mozilla.org/en/docs/nsIDownloadManager#addListener.28.29
[6] http://www.w3.org/TR/DOM-Level-2-Events/events.html#Events-EventException 